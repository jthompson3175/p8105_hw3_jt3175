---
title: "Homework 3"
author: "Julia Thompson"
date: "10/14/2019"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(viridis)
library(ggridges)
library(patchwork)
library(p8105.datasets)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

## Problem 1

Loading the _Instacart_ data and pulling a random sample for an example:

```{r}
data("instacart")
```

```{r}
# Look at a random person's order info for an example
set.seed(5)
instacart_sample = instacart %>% 
  filter(user_id == sample(pull(instacart, user_id), 1)) 
```

The instacart dataset contains detailed information from online grocery orders from 2017. It contains `r nrow(instacart)` observations and `r ncol(instacart)` variables. The dataset is organized by order. Within each order, there is one row for each item purchased, and these items have  an ID and description given by _product_id_ and _product_name_. Each customer and order have a unique ID, represented by the variables _user_id_ and _order_id_. The dataset also contai
ns IDs and names for each aisle and department from which every item came. There are `r n_distinct(pull(instacart, order_id))` distinct orders, made by `r n_distinct(pull(instacart, user_id))` different customers. Therefore, this dataset contains one order per customer. There is not a case of the same customer havind multiple orders here. 

For example, we can look at the order for the customer with _user_id_ `r min(pull(instacart_sample, user_id))`. That customer ordered `r nrow(instacart_sample)` items, including "`r min(pull(instacart_sample, product_name))`." Customer `r min(pull(instacart_sample, user_id))` had ordered `r sum(pull(instacart_sample, reordered))` of the `r nrow(instacart_sample)` items previously. 

```{r}
top_aisles = 
  instacart %>% 
  count(aisle) %>% 
  arrange(desc(n)) %>% 
  top_n(3, n)
```

There are `r n_distinct(pull(instacart, aisle_id))` aisles, and from the below table, we see that the top 3 aisles most items are ordered from are fresh vegetables, fresh fruits, and packaged vegetables fruits. Here, n represents the total number of items ordered from the respective aisle.

`r knitr::kable(top_aisles)`

```{r}
aisles_over_10k = 
  instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  arrange(desc(n))

ggplot(aisles_over_10k, aes(x = reorder(aisle, -n), y = n)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  labs(
    title = "Number of Items Ordered in Each Aisle",
    x = "Aisle",
    y = "Number of Items",
    caption = "Number of items ordered in each aisle with more than 10,000 items ordered"
  )
```

The above plot shows the number of items ordered in each aisle for aisles with more than 10,000 items ordered. We can see the same 3 items shown in the chart: _fresh vegetables_, _fresh fruits_, and _packaged vegetables fruits_. The majority of the aisles have less than 25,000 items ordered, with the _butter_ aisle having the least (in the > 10,000 category). I found it surprising that _water seltzer sparkling water_ ranked so high in number of items bought. 

```{r}
fav_items = instacart %>% 
  filter(aisle == c("baking ingredients","dog food care","packaged vegetables fruits")) %>% 
  group_by(aisle, product_name) %>% 
  summarize(
    n = n()
  ) %>% 
  top_n(3, n) %>% 
  arrange(aisle, n)
```

The below table shows the three most popular items in _baking ingredients_, _dog food care_, and _packaged vegetables fruits_, and the number of times each item was ordered. The _dog food care_ aisle's top 3 items were all much lower than the _baking ingredients_ and _packaged vegetables fruits_, which makes sense. 

`r knitr::kable(fav_items)`

```{r}
hr_of_day = instacart %>% 
  filter(product_name == c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(
    mean_hr = mean(order_hour_of_day)
  ) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hr
  ) %>% 
  rename(
    Sunday    = `0`, 
    Monday    = `1`, 
    Tuesday   = `2`, 
    Wednesday = `3`, 
    Thursday  = `4`, 
    Friday    = `5`, 
    Saturday  = `6`
  )
```

The below table shows the average hour of the day at which Pink Lady Apples and Coffee Ice Cream were ordered on each day of the week. It is important to note that we assumed 0 corresponded to Sunday in the dataset. We can see that people generally ordered Coffee Ice Cream a little bit later in the day than Pink Lady Apples, other than on Fridays.

`r knitr::kable(hr_of_day, digits = 2)`

## Problem 2

Loaded in the _brfss_smart2010_ data and cleaned it. Created a new dataset, _brfss_, that only contained the "Overall Health" topic and responses from "Poor" to "Excellent", which were organized in ascending order that way as a factor variable.

```{r}
data("brfss_smart2010")
```

```{r}
brfss = brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(topic == "Overall Health") %>% 
  mutate(
    response = factor(response, levels = c("Poor", "Fair", "Good", "Very good", "Excellent"))
  ) %>% 
  rename(state = locationabbr)
```

```{r}
brfss_2002 = brfss %>% 
  filter(year == 2002) %>% 
  group_by(state) %>% 
  summarize(
    n_dist = n_distinct(locationdesc)
  ) %>% 
  filter(n_dist >= 7)

brfss_2010 = brfss %>% 
  filter(year == 2010) %>% 
  group_by(state) %>% 
  summarize(
    n_dist = n_distinct(locationdesc)
  ) %>% 
  filter(n_dist >= 7)
```

In 2002, we see that there were `r nrow(brfss_2002)` states that were observed at 7 or more locations. The below table gives the states as well as the corresponding number of locations. 

`r knitr::kable(brfss_2002)`

In 2010, there were `r nrow(brfss_2010)` states that were observed at 7 or more locations. The below table shows the states and number of locations. We see that the number of states observed at 7 or more locations more than doubled between 2002 and 2010. 

`r knitr::kable(brfss_2010)`

Constructed a dataset that is limited to "Excellent" responses. It contains _year_, _state_, and _avg_data_value_, which is a variable that averages the _data_value_ across locations within a state. 

The below spaghetti plot shows the average of crude prevalence (%) over time within a state. For the most part, the crude prevalence is between 15% and 30%, with West Virginia having a noticable sharp decrease in crude prevalence in 2005 and again in 2009. Indianna also had a low crude prevalence in 2007. We also notice that DC has a high prevalence throughout, from 2002 to 2010.

```{r}
brfss_excellent = brfss %>% 
  filter(response == "Excellent") %>% 
  group_by(state, year) %>% 
  summarize(
    avg_data_value = mean(data_value)
  )

spaghetti = 
  ggplot(brfss_excellent, aes(x = year, y = avg_data_value, color = state)) +
  geom_line() +
  labs(
    title = "Data Value by State from 2002 to 2010",
    x = "Year",
    y = "Average Crude Prevalence (%)"
  )

spaghetti + 
  labs(color = "State") +
  guides(col = guide_legend(ncol = 14))
```

Below is a two-panel plot showing, for the years 2006, and 2010, the distribution of average crude prevalence (%) for responses "Poor" to "Excellent" among locations in NY State. We see for both 2006 and 2010, "Poor" responses had a low average crude prevalence for all of the counties. There is a general increasing trend for "Fair" and then "Good" responses, and then "Very good" and "Excellent" taper off. The trends are similar for most counties, though we do see crossover, particularly between "Good" and "Very good" for both 2006 and 2010.

```{r}
brfss_NY_2006 = brfss %>% 
  filter(state == "NY" & year == "2006") %>% 
  group_by(locationdesc, response) %>% 
  summarize(
    avg_data_value = mean(data_value)
  )

brfss_NY_2010 = brfss %>% 
  filter(state == "NY" & year == "2010") %>% 
  group_by(locationdesc, response) %>% 
  summarize(
    avg_data_value = mean(data_value)
  )

NY_2006 = 
  ggplot(brfss_NY_2006, aes(x = response, y = avg_data_value, color = locationdesc, group = locationdesc)) +
  geom_point() +
  geom_line() +
  labs(
    title = "Average Crude Prevalence for Response Among NY State Locations",
    subtitle = "2006",
    x = "Response",
    y = "Average Crude Prevalence (%)"
  ) 

NY_2006 = 
  NY_2006 +
  theme(legend.position = "none")


NY_2010 =
  ggplot(brfss_NY_2010, aes(x = response, y = avg_data_value, color = locationdesc, group = locationdesc)) +
  geom_point() +
  geom_line() +
  labs(
    title = "",
    subtitle = "2010",
    x = "Response",
    y = "Average Crude Prevalence (%)"
  )

NY_2010 = 
  NY_2010 + labs(color = "County")

(NY_2006/NY_2010)
```

## Problem 3

```{r}
acc_data = read_csv("./data/accel_data.csv")
```

Load, tidy, and otherwise wrangle the data. Your final dataset should include all originally observed variables and values; have useful variable names; include a weekday vs weekend variable; and encode data with reasonable variable classes. Describe the resulting dataset (e.g. what variables exist, how many observations, etc)

```{r}
acc_data_long = acc_data %>% 
  pivot_longer(
    cols = activity.1:activity.1440,
    names_to = "activity_number",
    values_to = "activity_counts",
    names_prefix = "activity.",
  ) %>% 
  rename(
    day_of_week = day
  ) %>% 
  mutate(
    is_weekend = (day_of_week == "Saturday" | day_of_week == "Sunday")
    )
```

Traditional analyses of accelerometer data focus on the total activity over the day. Using your tidied dataset, aggregate accross minutes to create a total activity variable for each day, and create a table showing these totals. Are any trends apparent?

```{r}
total_act = 
  acc_data_long %>% 
  group_by(day_id) %>% 
  summarize(
    total_activity = sum(activity_counts)
  ) 

knitr::kable(total_act)

# Look at trends

ggplot(total_act, aes(x = day_id, y = total_activity))+
  geom_point() +
  geom_line() +
  labs(
    title = "Total Activity Counts by Day",
    x = "Day",
    y = "Total Activity Counts"
  )
```

Accelerometer data allows the inspection activity over the course of the day. Make a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week. Describe in words any patterns or conclusions you can make based on this graph

```{r}
acc_by_dow = acc_data_long %>% 
  mutate(
    activity_number = as.numeric(as.character(activity_number))
  ) %>% 
  group_by(day_of_week, activity_number) %>% 
  summarize(
    avg_value = mean(activity_counts)
  )

plot_point = 
  ggplot(acc_by_dow, aes(x = activity_number, y = avg_value, color = day_of_week)) +
  geom_point() +
  scale_x_discrete(limit = c(240,480,720,960,1200,1440), 
                   labels=c("4", "8", "12", "16", "20", "24")) +
  labs(
    title = "Average 24 Hour Activity Time Courses for Each Day of the Week",
    x = "Activity Number (hrs)",
    y = "Average Activity Counts"
  )

plot_point + labs(color = "Day of the Week")

plot_smooth = 
  ggplot(acc_by_dow, aes(x = activity_number, y = avg_value, color = day_of_week)) +
  geom_smooth(se=FALSE) +
  scale_x_discrete(limit = c(240,480,720,960,1200,1440), 
                   labels=c("4", "8", "12", "16", "20", "24")) +
  labs(
    title = "Average 24 Hour Activity Time Courses for Each Day of the Week",
    x = "Activity Number (hrs)",
    y = "Average Activity Counts"
  )

plot_smooth + labs(color = "Day of the Week")

```

